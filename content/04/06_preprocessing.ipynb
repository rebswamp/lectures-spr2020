{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "## After this lecture, you\n",
    "\n",
    "- Can prepare categorical variables for `sklearn` models\n",
    "- Know that different imputation strategies exists and can use them\n",
    "- Know that standardizing continuous variables can improve your models\n",
    "- Know that you shouldn't apply preprocessing transformations with info from the testing dataset, that's called \"data leakage\" and is akin to letting your model \"seeing the future\" while training\n",
    "- Know that you should apply the **exact** transformations to the testing data that you applied to the training data before making predictions\n",
    "\n",
    "All of these can be accomplished by using `pipelines`.\n",
    "\n",
    "## Preprocessing categorical variables\n",
    "\n",
    "Depending on the variable you have, you can turn to\n",
    "- `DictVectorizer` is how you turn string categorical variables into usable numeric vars\n",
    "- `OneHotEncoder` takes array-like inputs instead of dicts\n",
    "\n",
    "Let's start by borrowing a clear example from [PDSH](https://jakevdp.github.io/PythonDataScienceHandbook/05.04-feature-engineering.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'price': 850, 'rooms': 4, 'neighborhood': 'Queen Anne'},\n",
       " {'price': 650, 'rooms': 3, 'neighborhood': 'Queen Anne'},\n",
       " {'price': 700, 'rooms': 1, 'neighborhood': 'Wallingford'},\n",
       " {'price': 650, 'rooms': 3, 'neighborhood': 'Wallingford'},\n",
       " {'price': 700, 'rooms': 3, 'neighborhood': 'Fremont'},\n",
       " {'price': 600, 'rooms': 2, 'neighborhood': 'Fremont'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    {'price': 850, 'rooms': 4, 'neighborhood': 'Queen Anne'},\n",
    "    {'price': 650, 'rooms': 3, 'neighborhood': 'Queen Anne'},\n",
    "    {'price': 700, 'rooms': 1, 'neighborhood': 'Wallingford'},\n",
    "    {'price': 650, 'rooms': 3, 'neighborhood': 'Wallingford'},\n",
    "    {'price': 700, 'rooms': 3, 'neighborhood': 'Fremont'},\n",
    "    {'price': 600, 'rooms': 2, 'neighborhood': 'Fremont'}\n",
    "]\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn` can't use `neighborhood` in a regression like `sm` could:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coefs from SM:\n",
      "neighborhood[Fremont]        650.0\n",
      "neighborhood[Queen Anne]     750.0\n",
      "neighborhood[Wallingford]    675.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd    \n",
    "from statsmodels.formula.api import ols as sm_ols\n",
    "print('The coefs from SM:')\n",
    "print(sm_ols('price ~ neighborhood - 1', data = pd.DataFrame(data)).fit().params)\n",
    "# \"\"-1\" means no intercept. Don't do this! It's here for illustration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we need to preprocess that data to run the same regression in `sklearn`. Depending on the variable you have, you can turn to\n",
    "- `DictVectorizer` is how you turn string categorical variables into usable numeric vars\n",
    "- `OneHotEncoder` takes array-like inputs instead of dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   1   0 850   4]\n",
      " [  0   1   0 650   3]\n",
      " [  0   0   1 700   1]\n",
      " [  0   0   1 650   3]\n",
      " [  1   0   0 700   3]\n",
      " [  1   0   0 600   2]] \n",
      "\n",
      "['neighborhood=Fremont', 'neighborhood=Queen Anne', 'neighborhood=Wallingford', 'price', 'rooms']\n",
      "Reg coefs:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([650., 750., 675.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an object (\"vec\") that can do the transform\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec = DictVectorizer(sparse=False, dtype=int) \n",
    "\n",
    "# apply vec with \".fit_transform\", save to new data obj\n",
    "data2 = vec.fit_transform(data) \n",
    "print(data2, '\\n')              \n",
    "print(vec.get_feature_names())  # can use .get_feature_names() to recover names\n",
    "\n",
    "# now we can repeat the regression here\n",
    "from sklearn.linear_model import LinearRegression\n",
    "print('Reg coefs:')\n",
    "LinearRegression(fit_intercept=False).fit(data2[:,:3],data2[:,3]).coef_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation / Missing Values\n",
    "\n",
    "_We talked [about imputation a bit before](https://ledatascifi.github.io/lectures-spr2020/02/05_outro.html#Dealing-With-Missing-Values) in the context of `pandas`. [These slides](https://github.com/matthewbrems/ODSC-missing-data-may-18/blob/master/Analysis%20with%20Missing%20Data.pdf) on missing data are quite good! [This article](https://www.geeksforgeeks.org/working-with-missing-data-in-pandas/) has examples too._\n",
    "\n",
    "Before modeling, you have to decide how to deal with missing values. You can \n",
    "1. Drop observations with any missing values, \n",
    "2. Impute missing values (mean, median, mode, interpolation, deduction, mean-of-group, etc), \n",
    "3. Or model the missing values explicitly (e.g. in a regression, as an incremental intercept but with no impact on the slope). \n",
    "\n",
    "What's the right choice? It depends. On the data, the domain, the question, and economic theory. My choices change from project to project. You might use a combination of these!\n",
    "\n",
    "**You should focus on the whys and hows of dealing with missing data rather than mechanics. (You can look up mechanics later.)** You should have some livecoding from the prior lecture showing imputation in `pandas`.\n",
    "\n",
    "`sklearn` comes with an `impute` class described in the [official docs](https://scikit-learn.org/stable/modules/impute.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan  0.  3.]\n",
      " [ 3.  7.  9.]\n",
      " [ 3.  5.  2.]\n",
      " [ 4. nan  6.]\n",
      " [ 8.  8.  1.]] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4.5, 0. , 3. ],\n",
       "       [3. , 7. , 9. ],\n",
       "       [3. , 5. , 2. ],\n",
       "       [4. , 5. , 6. ],\n",
       "       [8. , 8. , 1. ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# silly data\n",
    "import numpy as np\n",
    "X = np.array([[ np.nan, 0,   3  ],\n",
    "              [ 3,   7,   9  ],\n",
    "              [ 3,   5,   2  ],\n",
    "              [ 4,   np.nan, 6  ],\n",
    "              [ 8,   8,   1  ]])\n",
    "print(X,'\\n')\n",
    "\n",
    "# it's this easy:\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "imp.fit_transform(X) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`imp.fit_transform(X)` is the combination of `imp.fit(X)` and `imp.transform(X)`. \n",
    "\n",
    "If you have a train/test split, you shouldn't use `fit_transform`. Instead, use `imp.fit(X_train)` to get the means in the training sample and `imp.transform(X_test)` to apply those to the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "\n",
    "Effectively, this means that **continuous** variables should have a mean of 0 and a variance of one.\n",
    "\n",
    "The [`sklearn` documentation](https://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling) on this is quite good.\n",
    "\n",
    "> Standardization of datasets is a **common requirement for many machine learning estimators** implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance.\n",
    "\n",
    "Why does this matter? \"If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.\"\n",
    "\n",
    "**In other words: STANDARDIZATION WILL IMPROVE YOUR PREDICTIONS.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " X_scaled\n",
      " ---------------------------------------- \n",
      " [[ 0.         -1.22474487  1.33630621]\n",
      " [ 1.22474487  0.         -0.26726124]\n",
      " [-1.22474487  1.22474487 -1.06904497]] \n",
      "\n",
      " Mean of each var:\n",
      " ---------------------------------------- \n",
      " [0. 0. 0.] \n",
      "\n",
      " STD of each var:\n",
      " ---------------------------------------- \n",
      " [1. 1. 1.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a very simple example\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "X_scaled = preprocessing.scale(X_train)\n",
    "\n",
    "print(' X_scaled\\n',         '-'*40,'\\n',X_scaled,'\\n')\n",
    "print(' Mean of each var:\\n','-'*40,'\\n',X_scaled.mean(axis=0),'\\n')\n",
    "print(' STD of each var:\\n', '-'*40,'\\n',X_scaled.std(axis=0),'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn` can scale variables in many ways. Some alternative transforms are faster and some transform non-normal distributions into proto-normal distributions (which can improve the efficacy of many models).\n",
    "\n",
    "Visit (you guessed it!) [the documentation](https://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling) for more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data leakage: \n",
    "\n",
    "Now you know how to transform your data before training a model. You might be tempted to do something like:\n",
    "\n",
    "```python\n",
    "import #a bunch of sklearn stuff\n",
    "X, y = #load data\n",
    "X = transform(X) # imputation, encode cat vars, standardize\n",
    "\n",
    "# and then you either do these lines:\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=9,train_size=.8)\n",
    "model = # something\n",
    "model.fit(Xtrain, ytrain)\n",
    "y_predict = model.predict(Xtest) # using X2 (out-of-sample data), predict y2\n",
    "accuracy_score(ytest, y_predict)\n",
    "\n",
    "# or this:\n",
    "cross_validate(model,X,y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem here is that `transform(X)` used info from the **ENTIRE** dataset, including observations that ended up in `Xtest`!\n",
    "\n",
    "**This means that your cross-validation scores are unreliable.** They will be at the very least overoptimistic, and in some cases, result in models that are down-right completely invalid. \n",
    "\n",
    "### The absolute golden rule of prediction modeling is...\n",
    "\n",
    "**YOUR MODEL CAN'T HAVE ACCESS TO ANY DATA THAT IT WOULDN'T HAVE IN PRACTICE WHEN IT MAKES THE PREDICTION.**\n",
    "\n",
    "I know I already said that, and repetition is usually bad writing, but it must be said again. And again.\n",
    "\n",
    "### Data leakage can be tricky\n",
    "\n",
    "Here are some more examples:\n",
    "- The outcome variable is a predictor (implicitly or explicitly)\n",
    "- Predictor variables that are in response to the result (after the fact) or the possibility (anticipatory)\n",
    "- Predicting loan default, the data might include employee IDs for recent customer service contacts. But the most recent contact might be with trouble-loan specialists (because the firm anticipated possible default due to some other signal). Using that employee's customer contacts to predict default would add no value - the lender already knew to assign that employee!\n",
    "- The smell test - is it too good to be true? I've seen some asset pricing models with suspicious out-of-sample R2s. Predicting stock prices is hard! _The best OOS predictive R2 for individual stocks [in this paper](https://dachxiu.chicagobooth.edu/download/ML.pdf) is 1.80% per month._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The solution, or: Safety first, via Pipelines\n",
    "\n",
    "1. Be very familiar with the data and how it was collected and built \n",
    "1. Do your data prep within CV folds\n",
    "\n",
    "The second part of the solution is [relatively easy to implement in `sklearn`: PIPES](https://scikit-learn.org/stable/modules/compose.html)!\n",
    "- Pipelines make apply all steps to the data they receive\n",
    "- In `cross_validate`'s training fold, the entire pipeline is applied to the training data\n",
    "- In `cross_validate`'s testing fold, the saved transformations and model fits are applied to the test data\n",
    "\n",
    "Examples and walkthroughs:\n",
    "- [PDSH has an example of imputing, creating polynomial features, then fitting a regression in one line](https://jakevdp.github.io/PythonDataScienceHandbook/05.04-feature-engineering.html)\n",
    "    - [Another walkthrough with a pipeline](https://jakevdp.github.io/PythonDataScienceHandbook/05.06-linear-regression.html)\n",
    "    - [A pipeline used to optimize model parameters](https://jakevdp.github.io/PythonDataScienceHandbook/05.07-support-vector-machines.html)\n",
    "- [sklearn doc with details](https://scikit-learn.org/stable/modules/compose.html)\n",
    "- [sklearn doc with two walkthroughs](https://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.html), these walkthroughs use a pipeline to optimize a model's parameters\n",
    "\n",
    "Let's follow quickly this walkthrough on [scaling the iris data and building a classification model](https://chrisalbon.com/machine_learning/model_evaluation/cross_validation_pipeline/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.00099707, 0.00099754, 0.        , 0.0009973 , 0.        ]),\n",
       " 'score_time': array([0.00099659, 0.        , 0.00099778, 0.        , 0.0009973 ]),\n",
       " 'test_score': array([0.96666667, 0.96666667, 0.96666667, 0.93333333, 1.        ])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import svm\n",
    "\n",
    "iris = load_iris() # data\n",
    "\n",
    "# set up the pipeline, which will, given a set of observations \n",
    "# 1. fit and apply these steps to the training fold\n",
    "# 2. in the testing fold, apply the transform and model to predict (no estimation)\n",
    "\n",
    "classifier_pipeline = make_pipeline(preprocessing.StandardScaler(), svm.SVC(C=1))\n",
    "\n",
    "# ok, go!\n",
    "scores = cross_validate(classifier_pipeline, iris.data, iris.target, cv=5)\n",
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
