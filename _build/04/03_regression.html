---
redirect_from:
  - "/04/03-regression"
interact_link: content/04/03_regression.ipynb
kernel_name: python3
kernel_path: content/04
has_widgets: false
title: |-
  Regression - Mechanics and Interpretation
pagenum: 22
prev_page:
  url: /04/02_git_collaboration.html
next_page:
  url: /04/04_introML.html
suffix: .ipynb
search: beta y variables log regression x hat align ideal variable text data carat good diamonds cut price begin end r fair model value media giphy categorical values p line increase dummy com fit sklearn t e g its different above carats diamond coefficient uparrow avg coefficients u get including because here cases higher very change deviation statsmodels gif same example say unit premium standard significant html aka stat not relationship only nice method linear better std blue focus results qualitative term changes does might points estimate predicted estimation those sum regressions tables im much why shows larger means br average

comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

    <main class="jupyter-page">
    <div id="page-info"><div id="page-title">Regression - Mechanics and Interpretation</div>
</div>
    <div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Regression---Mechanics-and-Interpretation">Regression - Mechanics and Interpretation<a class="anchor-link" href="#Regression---Mechanics-and-Interpretation"> </a></h1><h2 id="Housecleaning">Housecleaning<a class="anchor-link" href="#Housecleaning"> </a></h2><ol>
<li>Assignment 5 review</li>
<li>All project instructions moved to <a href="https://ledatascifi.github.io/assignments/project.html">project page</a> under assignments tab</li>
</ol>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Objectives">Objectives<a class="anchor-link" href="#Objectives"> </a></h2><ol>
<li>You can fit a regression with <code>statsmodels</code> or <code>sklearn</code></li>
<li>You can view the results visually or numerically of your model with either </li>
<li>You can interpret the mechanical meaning of the coefficients in a regression<ul>
<li>continuous variables</li>
<li>categorical a.k.a qualitative variables with two values (aka "dummy" or "binary" variables)</li>
<li>categorical a.k.a qualitative variables with more than values (aka "fixed effects")</li>
<li>how an interaction term changes interpretation</li>
</ul>
</li>
<li>You understand what a t-stat / p-value does and does not tell you</li>
<li>You can measure the goodness of fit on a regression</li>
<li>You are aware of common regression analysis pitfalls and disasters</li>
</ol>
<p><img src="https://media.giphy.com/media/yoJC2K6rCzwNY2EngA/giphy.gif" alt=""></p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Regression">Regression<a class="anchor-link" href="#Regression"> </a></h2><h3 id="Basics-and-notation">Basics and notation<a class="anchor-link" href="#Basics-and-notation"> </a></h3><ul>
<li><strong>Regression</strong> is the single most important tool at the econometrician's disposal</li>
<li><strong>Regression analysis</strong> is concerned with the description and evaluation of the relationship between a variable typically called the dependent variable, and one or more other variables, typically called the independent or explanatory variables.</li>
<li>Alternative vocabulary:</li>
</ul>
<table>
<thead><tr>
<th style="text-align:left">y</th>
<th style="text-align:left">x</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">dependent variable</td>
<td style="text-align:left">independent variables</td>
</tr>
<tr>
<td style="text-align:left">regressand</td>
<td style="text-align:left">regressors</td>
</tr>
<tr>
<td style="text-align:left">effect variable</td>
<td style="text-align:left">causal variables</td>
</tr>
<tr>
<td style="text-align:left">explained variable</td>
<td style="text-align:left">explanatory variables</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">features</td>
</tr>
</tbody>
</table>
<h3 id="The-regression-&quot;model&quot;-and-terminology">The regression "model" and terminology<a class="anchor-link" href="#The-regression-&quot;model&quot;-and-terminology"> </a></h3><p>If you want to describe the data (the scatterplot below, e.g.) you might fit a straight line</p>
\begin{align} 
y=a+\beta x
\end{align}<p></p>
<p>But that line can't exactly fit all the points of data (it's impossible to find all determinants of $y$ and $y$ may be mismeasured), so you need to account for the discrepancies, and so we add a "error term" or "disturbance" denoted by $u$</p>
\begin{align} 
y=a+\beta x+u
\end{align}<p></p>
<!-- explain reasons the u term exists -->

<p>Now, we want to estimate $a$ and $\beta$ to best "fit" the data. Imagine you pick/estimate $\hat{a}$ and $\hat{\beta}$  to fit the data. If you apply that to all the X data points like $\hat{a} + \hat{\beta}x$ , then you get a <strong>predicted value for $y$</strong>:</p>
\begin{align} 
\hat{y} = \hat{a} + \hat{\beta}x
\end{align}<p></p>
<p>We call $\hat{y}$ the "fitted values" or the "predicted values" of y. And the difference between each actual $y$ and the predicted $\hat{y}$ is called the <strong>residual</strong> or <strong>error</strong>:</p>
\begin{align} 
y-\hat{y} = \hat{u} = \text{"residual" aka "error"}
\end{align}<p></p>
<p>The goal of estimation (any estimation, including regression) is to make these errors as "small" as possible. Regression is aka'ed as "Ordinary Least Squares" which as it sounds, takes those errors, squares them, adds those up, and minimizes the sum</p>
\begin{align} 
   \min &amp; \sum(y-\hat{y})^2 
 = \min &amp; \sum(y-\hat{a} + \hat{\beta}x) 
\end{align}<p></p>
<p>So, to combine this with the "Modeling Intro" lecture, regression follows the same steps as any estimation:</p>
<ol>
<li>Select a model. _In a regression, the model is a line (if you only have 1 X variable) or a hyperplane (if you have many X variables).)</li>
<li>Select a loss function. <em>In a regression, it's the sum of squared errors.</em></li>
<li>Minimize the loss. <em>You can solve for the minimum analytically (take the derivative, ...) or numerically (gradient descent). Our python packages below handle this for you.</em></li>
</ol>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Running-regressions---codebook">Running regressions - codebook<a class="anchor-link" href="#Running-regressions---codebook"> </a></h3><ul>
<li>3 ways to fit a regression</li>
<li>Which ever you choose, you need to be able to extract the coefficients, t-stats, R2, AR2, residuals, and predicted values.</li>
</ul>
<p>Let's get our hands dirty quickly by loading data... and do a demo on diamonds</p>
<p><img src="https://media.giphy.com/media/piXrzDejeWIM/giphy.gif" alt=""></p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># load some data to practice regressions</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">diamonds</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;diamonds&#39;</span><span class="p">)</span>

<span class="c1"># this alteration is not strictly necessary to practice a regression</span>
<span class="c1"># but we use this in livecoding</span>
<span class="n">diamonds2</span> <span class="o">=</span> <span class="p">(</span><span class="n">diamonds</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;carat &lt; 2.5&#39;</span><span class="p">)</span>               <span class="c1"># censor/remove outliers</span>
            <span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">lprice</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">diamonds</span><span class="p">[</span><span class="s1">&#39;price&#39;</span><span class="p">]))</span>  <span class="c1"># log transform price</span>
            <span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">lcarat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">diamonds</span><span class="p">[</span><span class="s1">&#39;carat&#39;</span><span class="p">]))</span>  <span class="c1"># log transform carats</span>
            <span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">ideal</span> <span class="o">=</span> <span class="n">diamonds</span><span class="p">[</span><span class="s1">&#39;cut&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;Ideal&#39;</span><span class="p">)</span> 
             
             <span class="c1"># some regression packages want you to explicitly provide </span>
             <span class="c1"># a variable for the constant</span>
            <span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">const</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>                           
            <span class="p">)</span>  
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We will cover examples with <code>statsmodels</code> and <code>sklearn</code>.</p>
<ul>
<li>I prefer <code>statsmodels</code> when I want to look at tables of results and find specifying the model easier sometimes</li>
<li>I prefer <code>sklearn</code> when I'm using regression within a prediction/ML exercise because <code>sklearn</code> has nice tools to construct "training" and "testing" samples</li>
</ul>
<h4 id="Regression-method-1:--statsmodels.api:">Regression method 1:  <code>statsmodels.api</code>:<a class="anchor-link" href="#Regression-method-1:--statsmodels.api:"> </a></h4>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>  

<span class="n">y</span> <span class="o">=</span> <span class="n">diamonds2</span><span class="p">[</span><span class="s1">&#39;lprice&#39;</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">diamonds2</span><span class="p">[[</span><span class="s1">&#39;const&#39;</span><span class="p">,</span><span class="s1">&#39;lcarat&#39;</span><span class="p">]]</span>

<span class="n">model1</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">X</span><span class="p">)</span>                <span class="c1"># pick model type and specify model features</span>
<span class="n">results1</span> <span class="o">=</span> <span class="n">model1</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>             <span class="c1"># estimate / fit</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results1</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>           <span class="c1"># view results </span>
<span class="n">y_predicted1</span> <span class="o">=</span> <span class="n">results1</span><span class="o">.</span><span class="n">predict</span><span class="p">()</span>   <span class="c1"># get the predicted results</span>
<span class="n">residuals1</span> <span class="o">=</span> <span class="n">results1</span><span class="o">.</span><span class="n">resid</span>         <span class="c1"># get the residuals</span>
<span class="c1">#residuals1 = y - y_predicted1      # another way to get the residuals</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                 lprice   R-squared:                       0.933
Model:                            OLS   Adj. R-squared:                  0.933
Method:                 Least Squares   F-statistic:                 7.542e+05
Date:                Thu, 26 Mar 2020   Prob (F-statistic):               0.00
Time:                        08:02:36   Log-Likelihood:                -4073.2
No. Observations:               53797   AIC:                             8150.
Df Residuals:                   53795   BIC:                             8168.
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          8.4525      0.001   6193.432      0.000       8.450       8.455
lcarat         1.6819      0.002    868.465      0.000       1.678       1.686
==============================================================================
Omnibus:                      775.052   Durbin-Watson:                   1.211
Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1334.265
Skew:                           0.106   Prob(JB):                    1.85e-290
Kurtosis:                       3.742   Cond. No.                         2.10
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Regression-method-2:--statsmodels.formula.api:">Regression method 2:  <code>statsmodels.formula.api</code>:<a class="anchor-link" href="#Regression-method-2:--statsmodels.formula.api:"> </a></h4><p>I like this because you can write the equation out more naturally, and it allows you to easily include categorical variables. <a href="https://stackoverflow.com/questions/50733014/linear-regression-with-dummy-categorical-variables">See here</a> for an example of that.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_remove_output">

<div class="cell border-box-sizing code_cell rendered tag_remove_output">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">statsmodels.formula.api</span> <span class="kn">import</span> <span class="n">ols</span> <span class="k">as</span> <span class="n">sm_ols</span>

<span class="n">model2</span>   <span class="o">=</span> <span class="n">sm_ols</span><span class="p">(</span><span class="s1">&#39;lprice ~ lcarat&#39;</span><span class="p">,</span>  <span class="c1"># specify model (you don&#39;t need to include the constant!)</span>
                  <span class="n">data</span><span class="o">=</span><span class="n">diamonds2</span><span class="p">)</span>
<span class="n">results2</span> <span class="o">=</span> <span class="n">model2</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>               <span class="c1"># estimate / fit</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results2</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>             <span class="c1"># view results ... identical to before</span>

<span class="c1"># the prediction and residual and plotting are the exact same</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Regression-method-3:--sklearn:">Regression method 3:  <code>sklearn</code>:<a class="anchor-link" href="#Regression-method-3:--sklearn:"> </a></h4><p><code>sklearn</code> is pretty similar but it doesn't have the nice summary tables:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">model3</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>         
<span class="n">results3</span> <span class="o">=</span> <span class="n">model3</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;INTERCEPT:&#39;</span><span class="p">,</span> <span class="n">results3</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>  <span class="c1"># yuck, definitely uglier</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;COEFS:&#39;</span><span class="p">,</span> <span class="n">results3</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span> 

<span class="c1"># the fitted predictions/residuals are just a little diff</span>
<span class="n">y_predicted3</span> <span class="o">=</span> <span class="n">results3</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> 
<span class="n">residuals3</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">y_predicted3</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>INTERCEPT: 8.452511832951718
COEFS: [0.         1.68193567]
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>That's so much uglier. Why use <code>sklearn</code>?</p>
<p>Because <code>sklearn</code> is the go-to for training models using more sophisticated ML ideas (which we will talk about some later in the course!). Two nice walkthroughs:</p>
<ul>
<li><a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.06-linear-regression.html">This guide from the PythonDataScienceHandbook</a> (you can use different data though)</li>
<li>The "Linear Regression" section <a href="https://becominghuman.ai/linear-regression-in-python-with-pandas-scikit-learn-72574a2ec1a5">here</a> shows how you can run regressions on training samples and test them out of sample</li>
</ul>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Plotting-the-regression-fit">Plotting the regression fit<a class="anchor-link" href="#Plotting-the-regression-fit"> </a></h3><p>This works with all of the above methods easily.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># let&#39;s plot our data with the OLS predicted fit</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;lcarat&#39;</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="s1">&#39;lprice&#39;</span><span class="p">,</span><span class="n">data</span><span class="o">=</span><span class="n">diamonds2</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">1000</span><span class="p">))</span> <span class="c1"># sampled just to avoid overplotting</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">diamonds2</span><span class="p">[</span><span class="s1">&#39;lcarat&#39;</span><span class="p">],</span><span class="n">y</span><span class="o">=</span><span class="n">y_predicted1</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>

<span class="c1"># compare this to the built-in sns produces</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">regplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;lcarat&#39;</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="s1">&#39;lprice&#39;</span><span class="p">,</span><span class="n">data</span><span class="o">=</span><span class="n">diamonds2</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">1000</span><span class="p">),</span>
            <span class="n">line_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;color&#39;</span><span class="p">:</span><span class="s1">&#39;red&#39;</span><span class="p">})</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Which is the same fit sns will give&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../images/04/03_regression_13_0.png"
>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Text(0.5, 1.0, &#39;Which is the same fit sns will give&#39;)</pre>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../images/04/03_regression_13_2.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Example:-Including-dummy-variables">Example: Including dummy variables<a class="anchor-link" href="#Example:-Including-dummy-variables"> </a></h3><p>Suppose you started by estimating the price of diamonds as a function of carats</p>
\begin{align} 
\log(\text{price})=a+\beta_0 \log(\text{carat}) +u
\end{align}<p></p>
<p>but you realize it will be different for ideal cut diamonds. That is, a 1 carat diamond might cost $1,000, but if it's ideal, it's an extra $500 dollars.</p>
\begin{align} 
\log(\text{price})=
    \begin{cases}
      a+\beta_0 \log(\text{carat}) + \beta_1 +u, &amp; \text{if ideal cut}  \\
      a+\beta_0 \log(\text{carat}) +u, &amp; \text{otherwise}
    \end{cases} 
\end{align}<p></p>
<p>Notice that $\beta_0$ in this model are the same. Here is how you run this test:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># ideal is a dummy variable = 1 if ideal and 0 if not ideal</span>
<span class="n">model_ideal</span>   <span class="o">=</span> <span class="n">sm_ols</span><span class="p">(</span><span class="s1">&#39;lprice ~ lcarat + ideal&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">diamonds2</span><span class="p">)</span>
<span class="n">results_ideal</span> <span class="o">=</span> <span class="n">model_ideal</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>               <span class="c1"># estimate / fit</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results_ideal</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>             <span class="c1"># view results </span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                 lprice   R-squared:                       0.936
Model:                            OLS   Adj. R-squared:                  0.936
Method:                 Least Squares   F-statistic:                 3.914e+05
Date:                Thu, 26 Mar 2020   Prob (F-statistic):               0.00
Time:                        08:02:40   Log-Likelihood:                -3136.4
No. Observations:               53797   AIC:                             6279.
Df Residuals:                   53794   BIC:                             6306.
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
=================================================================================
                    coef    std err          t      P&gt;|t|      [0.025      0.975]
---------------------------------------------------------------------------------
Intercept         8.4182      0.002   5415.779      0.000       8.415       8.421
ideal[T.True]     0.1000      0.002     43.662      0.000       0.096       0.105
lcarat            1.6963      0.002    878.286      0.000       1.692       1.700
==============================================================================
Omnibus:                      794.680   Durbin-Watson:                   1.241
Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1394.941
Skew:                           0.101   Prob(JB):                    1.24e-303
Kurtosis:                       3.763   Cond. No.                         2.67
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Example:-Including-categorical-variables">Example: Including categorical variables<a class="anchor-link" href="#Example:-Including-categorical-variables"> </a></h3><p>Method 2 also processes categorical variables easily!</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sm_ols</span><span class="p">(</span><span class="s1">&#39;lprice ~ cut&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">diamonds2</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>         <td>lprice</td>      <th>  R-squared:         </th> <td>   0.018</td> 
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.017</td> 
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   239.6</td> 
</tr>
<tr>
  <th>Date:</th>             <td>Thu, 26 Mar 2020</td> <th>  Prob (F-statistic):</th> <td>2.66e-204</td>
</tr>
<tr>
  <th>Time:</th>                 <td>08:02:40</td>     <th>  Log-Likelihood:    </th> <td> -76477.</td> 
</tr>
<tr>
  <th>No. Observations:</th>      <td> 53797</td>      <th>  AIC:               </th> <td>1.530e+05</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td> 53792</td>      <th>  BIC:               </th> <td>1.530e+05</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    
</tr>
</table>
<table class="simpletable">
<tr>
          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>        <td>    8.0688</td> <td>    0.025</td> <td>  320.067</td> <td> 0.000</td> <td>    8.019</td> <td>    8.118</td>
</tr>
<tr>
  <th>cut[T.Good]</th>      <td>   -0.2328</td> <td>    0.029</td> <td>   -8.025</td> <td> 0.000</td> <td>   -0.290</td> <td>   -0.176</td>
</tr>
<tr>
  <th>cut[T.Ideal]</th>     <td>   -0.4319</td> <td>    0.026</td> <td>  -16.536</td> <td> 0.000</td> <td>   -0.483</td> <td>   -0.381</td>
</tr>
<tr>
  <th>cut[T.Premium]</th>   <td>   -0.1241</td> <td>    0.027</td> <td>   -4.663</td> <td> 0.000</td> <td>   -0.176</td> <td>   -0.072</td>
</tr>
<tr>
  <th>cut[T.Very Good]</th> <td>   -0.2732</td> <td>    0.027</td> <td>  -10.188</td> <td> 0.000</td> <td>   -0.326</td> <td>   -0.221</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>14708.225</td> <th>  Durbin-Watson:     </th> <td>   0.049</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>2503.627</td>
</tr>
<tr>
  <th>Skew:</th>           <td> 0.118</td>   <th>  Prob(JB):          </th> <td>    0.00</td>
</tr>
<tr>
  <th>Kurtosis:</th>       <td> 1.970</td>   <th>  Cond. No.          </th> <td>    15.0</td>
</tr>
</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Example:-Including-interaction-terms">Example: Including interaction terms<a class="anchor-link" href="#Example:-Including-interaction-terms"> </a></h3><p>Suppose that an ideal cut diamond doesn't just add a fixed dollar value to the diamond. Perhaps it also changes the value of having a larger diamond. You might say that</p>
<ul>
<li>A high quality cut is even more valuable for a larger diamond than it is for a small diamond. ("A great cut makes a diamond sparkle, but it's hard to see sparkle on a tiny diamond no matter what.")</li>
<li>In other words, the effect of carats depends on the cut and visa versa</li>
<li>In other words, "the cut variable <strong>interacts</strong> with the carat variable"</li>
<li>So you might say that, "a better cut changes the slope/coefficient of carat"</li>
<li>Or equivalently, "a better cut changes the return on a larger carat"</li>
</ul>
<p>Graphically, it's easy to see, as <code>sns.lmplot</code> by default gives each cut a unique slope on carats:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># notice how these lines have different slopes?</span>
<span class="n">subsample_of_equal_amounts</span> <span class="o">=</span> <span class="n">diamonds2</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;cut in [&quot;Ideal&quot;,&quot;Fair&quot;]&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;cut&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">400</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lmplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">subsample_of_equal_amounts</span><span class="p">,</span>
           <span class="n">y</span><span class="o">=</span><span class="s1">&#39;lprice&#39;</span><span class="p">,</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;lcarat&#39;</span><span class="p">,</span><span class="n">hue</span><span class="o">=</span><span class="s1">&#39;cut&#39;</span><span class="p">,</span><span class="n">ci</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> 
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&lt;seaborn.axisgrid.FacetGrid at 0x29128c19808&gt;</pre>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../images/04/03_regression_19_1.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Those two different lines above are estimated by</p>
\begin{align} 
\log(\text{price})= a+ \beta_0 \log(\text{carat}) + \beta_1 \text{Ideal} + \beta_2\log(\text{carat})\cdot \text{Ideal}
\end{align}<p>If you plug in 1 for $ideal$, you get the line for ideal diamonds as</p>
\begin{align} 
\log(\text{price})= a+ \beta_1 +(\beta_0 + \beta_2) \log(\text{carat}) 
\end{align}<p>If you plug in 0 for $ideal$, you get the line for fair diamonds as</p>
\begin{align} 
\log(\text{price})= a+ \beta_0 \log(\text{carat}) 
\end{align}<p>So, by including that interaction term, you get that the slope on carats is different for Ideal than Fair diamonds.</p>
<p><code>sm</code> will estimate that nicely:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># you can include the interaction of x and z by adding &quot;+x*z&quot; in the spec, like:</span>
<span class="n">sm_ols</span><span class="p">(</span><span class="s1">&#39;lprice ~ lcarat + ideal + lcarat*ideal&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">subsample_of_equal_amounts</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>         <td>lprice</td>      <th>  R-squared:         </th> <td>   0.888</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.888</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   2113.</td>
</tr>
<tr>
  <th>Date:</th>             <td>Thu, 26 Mar 2020</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> 
</tr>
<tr>
  <th>Time:</th>                 <td>08:02:40</td>     <th>  Log-Likelihood:    </th> <td> -152.23</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   800</td>      <th>  AIC:               </th> <td>   312.5</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   796</td>      <th>  BIC:               </th> <td>   331.2</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
            <td></td>              <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>            <td>    8.1804</td> <td>    0.015</td> <td>  545.561</td> <td> 0.000</td> <td>    8.151</td> <td>    8.210</td>
</tr>
<tr>
  <th>ideal[T.True]</th>        <td>    0.3308</td> <td>    0.025</td> <td>   13.000</td> <td> 0.000</td> <td>    0.281</td> <td>    0.381</td>
</tr>
<tr>
  <th>lcarat</th>               <td>    1.5102</td> <td>    0.036</td> <td>   42.452</td> <td> 0.000</td> <td>    1.440</td> <td>    1.580</td>
</tr>
<tr>
  <th>lcarat:ideal[T.True]</th> <td>    0.1826</td> <td>    0.044</td> <td>    4.111</td> <td> 0.000</td> <td>    0.095</td> <td>    0.270</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>15.462</td> <th>  Durbin-Watson:     </th> <td>   2.066</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  20.609</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.210</td> <th>  Prob(JB):          </th> <td>3.35e-05</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 3.664</td> <th>  Cond. No.          </th> <td>    6.77</td>
</tr>
</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This shows that a 1% increase in carats is associated with a 1.47% increase in price for fair diamonds, but a 1.78% increase for ideal diamonds (1.47+0.28).</p>
<p>Thus: The return on carats is different (and higher) for better cut diamonds!</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Mechanical-interpretation-of-regression-coefficients">Mechanical interpretation of regression coefficients<a class="anchor-link" href="#Mechanical-interpretation-of-regression-coefficients"> </a></h2><h3 id="If-X-is-a-continuous-variable">If X is a continuous variable<a class="anchor-link" href="#If-X-is-a-continuous-variable"> </a></h3><table>
<thead><tr>
<th style="text-align:left">If the model is..................</th>
<th style="text-align:left">then $\beta$ means (approx. in log cases)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">$y=a+\beta X$</td>
<td style="text-align:left">If $X \uparrow $ 1 unit, then $y \uparrow$ by $\beta$ units</td>
</tr>
<tr>
<td style="text-align:left">$\log y=a+\beta X$</td>
<td style="text-align:left">If $X \uparrow $ 1 unit, then $y \uparrow$ by about $100*\beta$%. <br> <em>(Exact: $100*(\exp(\beta)-1)$)</em></td>
</tr>
<tr>
<td style="text-align:left">$y=a+\beta \log X$</td>
<td style="text-align:left">If $X \uparrow $ 1%, then $y \uparrow$ by about $\beta / 100$ units</td>
</tr>
<tr>
<td style="text-align:left">$\log y=a+\beta \log X$</td>
<td style="text-align:left">If $X \uparrow $ 1%, then $y \uparrow$ by $\beta$%</td>
</tr>
</tbody>
</table>
<h3 id="If-X-is-a-binary-variable">If X is a binary variable<a class="anchor-link" href="#If-X-is-a-binary-variable"> </a></h3><p>This is a categorical or qualitative variable with two values (aka "dummy"). E.g. gender in Census data, and <code>"ideal"</code> above.</p>
<table>
<thead><tr>
<th style="text-align:left">If the model is..................</th>
<th style="text-align:left">then $\beta$ means</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">$y=a+\beta X$</td>
<td style="text-align:left">$y$ is $\beta$ units higher for cases when $X=1$ than when $X=0$.</td>
</tr>
<tr>
<td style="text-align:left">$\log y=a+\beta X$</td>
<td style="text-align:left">$y$ is about $\beta$ % higher for cases when $X=1$ than when $X=0$.</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="If-X-is-a-categorical-variable">If X is a categorical variable<a class="anchor-link" href="#If-X-is-a-categorical-variable"> </a></h3><p>This is a categorical or qualitative variable with more than two values. E.g. <code>"cut"</code> in the diamonds dataset. In a regression, this ends up looking like</p>
\begin{align} 
\log(\text{price})=
    \begin{cases}
      a, &amp; \text{if cut is fair} \\
      a +\beta_{Good}, &amp; \text{if cut is Good} \\
      a +\beta_{Very Good}, &amp; \text{if cut is Very Good} \\
      a +\beta_{Premium}, &amp; \text{if cut is Premium} \\
      a +\beta_{Ideal}, &amp; \text{if cut is Ideal} 
          \end{cases} 
\end{align}<p></p>
<p>So you do is take the cut variable <code>cut={Fair,Good,Very Good,Premium,Ideal}</code> and turn it into a dummy variable for "Good", a dummy variable for "Very Good", and so on.</p>
<p><strong>Warning: You don't create a dummy variable for all the categories! That's why I skipped "Fair" in the prior paragraph.</strong></p>
<p>Then...</p>
<table>
<thead><tr>
<th style="text-align:left">$\beta$.....</th>
<th style="text-align:left">means</th>
<th style="text-align:left">or</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">$\beta_{Good}$</td>
<td style="text-align:left">The average log(price) for Good diamonds is $\beta_{Good}$ higher than Fair diamonds.</td>
<td style="text-align:left">$avg_{Good}-avg_{Fair}$</td>
</tr>
<tr>
<td style="text-align:left">$\beta_{Very Good}$</td>
<td style="text-align:left">The average log(price) for Good diamonds is $\beta_{Very Good}$ higher than Fair diamonds.</td>
<td style="text-align:left">$avg_{Very Good}-avg_{Fair}$</td>
</tr>
<tr>
<td style="text-align:left">$\beta_{Premium}$</td>
<td style="text-align:left">The average log(price) for Good diamonds is $\beta_{Premium}$ higher than Fair diamonds.</td>
<td style="text-align:left">$avg_{Premium}-avg_{Fair}$</td>
</tr>
<tr>
<td style="text-align:left">$\beta_{Ideal}$</td>
<td style="text-align:left">The average log(price) for Good diamonds is $\beta_{Ideal}$ higher than Fair diamonds.</td>
<td style="text-align:left">$avg_{Ideal}-avg_{Fair}$</td>
</tr>
</tbody>
</table>
<p><strong>THE MAIN THING TO REMEMBER IS THAT $\beta_{value}$ COMPARES THAT $value$ TO THE <em>OMITTED</em> CATEGORY!</strong></p>
<p>So, compare the regression above which reports that $a$ (intercept) = 8.0688 and $\beta_{Good}$ equals -0.2328 to this:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">diamonds2</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;cut&#39;</span><span class="p">)[</span><span class="s1">&#39;lprice&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="c1"># avg lprice by cut</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>cut
Fair         8.068832
Good         7.836076
Ideal        7.636921
Premium      7.944690
Very Good    7.795675
Name: lprice, dtype: float64</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>8.0688 + (-0.2328) = 7.8360!!!</p>
<p>The nice part of <code>sm</code> is that it automatically "creates" the dummy variables and omits a value for you!</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Interpreting-with-multiple-variables-(THE-WOES-THEREOF)">Interpreting with multiple variables (THE WOES THEREOF)<a class="anchor-link" href="#Interpreting-with-multiple-variables-(THE-WOES-THEREOF)"> </a></h3><p>If you have multiple (up to N controls):</p>
\begin{align}
y = a +\beta_0 X_0 + \beta_1 X_1+ ...+\beta_N X_N+ u
\end{align}<ul>
<li>$\beta_1$ estimates the expected change in Y for a 1 unit increase in $X_1$ (as we covered above).</li>
<li>But predictors usually change together!!!</li>
<li><p>If Y = number of tackles by a football player in a year, $W$ is weight, and $H$ is height, and we estimate that</p>
<p>\begin{align}
  y = a +\hat{0.5} W + \hat{-0.1} H
  \end{align}</p>
<p>How do you interpret $\hat{\beta_1} &lt; 0 $ on H?</p>
</li>
</ul>
<p>If categorical variables are included (e.g. year or industry), when thinking about the OTHER variables, you can think</p>

<pre><code>- "comparing same firms within the same year..." 
- "comparing firms in the same industry, controlling for industry factors..."</code></pre>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="A-useful-trick:-Comparing-the-size-of-two-coefficients">A useful trick: Comparing the size of two coefficients<a class="anchor-link" href="#A-useful-trick:-Comparing-the-size-of-two-coefficients"> </a></h3><p>Earlier, we estimated that $\log price = \hat{8.41} + \hat{1.69} \log carat + \hat{0.10} ideal$.</p>
<p>So... I have questions:</p>
<ul>
<li>Does that mean that $\log carat$ has a 17 times larger impact than $ideal$ in terms of price impact? </li>
<li>How do we compare those magnitudes?</li>
<li>More generally, how do we compare the magnitudes of any 2 control variables?</li>
</ul>
<p>To which, I'd say that how "big" a coefficient is depends on the variable!</p>
<ul>
<li>For some variables, an increase of 1 unit is common (e.g. our $ideal$ dummy is one 40% of the time)</li>
<li>For some variables, an increase of 1 unit is rare (e.g. leverage)</li>
<li><strong>$\rightarrow$ the meaning of the coefficient's magnitude <em>depends on the corresponding variable's variation!</em></strong></li>
<li>$\rightarrow$ so change variables so that a 1 unit increase implies the same amount of movement</li>
<li><strong>$\rightarrow$ Solution: scale variables by their standard deviation!*</strong></li>
</ul>
<p>* <em>(Only continuous variables! Don't do this for dummy variables or categorical variables)</em></p>
<p>Here is that solution in action:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">standardize</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="n">x</span><span class="o">/</span><span class="n">x</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="c1"># normalize(df[&#39;x&#39;]) will divide all &#39;x&#39; by the std deviation of &#39;x&#39;</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Divide lcarat by its std dev:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sm_ols</span><span class="p">(</span><span class="s1">&#39;lprice ~ lcarat + ideal&#39;</span><span class="p">,</span> 
       <span class="c1"># for **just** this regression, divide </span>
       <span class="n">data</span><span class="o">=</span><span class="n">diamonds2</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">lcarat</span> <span class="o">=</span> <span class="n">standardize</span><span class="p">(</span><span class="n">diamonds2</span><span class="p">[</span><span class="s1">&#39;lcarat&#39;</span><span class="p">]))</span> 
       <span class="c1"># this doesn&#39;t change the diamonds2 data permanently, so the next time you call on</span>
       <span class="c1"># diamonds2, you can use lcarat as if nothing changed. if you want to repeat this</span>
       <span class="c1"># a bunch, you might instead create and save a permanent variable called &quot;lcarat_std&quot;</span>
       <span class="c1"># where &quot;_std&quot; indicates that you divided it by the std dev.</span>
      <span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">The original reg:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sm_ols</span><span class="p">(</span><span class="s1">&#39;lprice ~ lcarat + ideal&#39;</span><span class="p">,</span><span class="n">data</span><span class="o">=</span><span class="n">diamonds2</span> <span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Divide lcarat by its std dev:

Intercept        8.418208
ideal[T.True]    0.100013
lcarat           0.985628
dtype: float64


The original reg:

Intercept        8.418208
ideal[T.True]    0.100013
lcarat           1.696259
dtype: float64
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>So a 1 standard deviation increase in $\log carat$ is associated with a 0.98% increase in price. Compared to $ideal$, we can say that a reasonable variation in carats is associated with a price increase about 10 times the size.</strong></p>
<p>Notably, 0.98 is only 58% of the original coefficient (1.69).</p>
<h4 id="Q:-Why-is-it-58%-of-the-previous-coefficient?">Q: Why is it 58% of the previous coefficient?<a class="anchor-link" href="#Q:-Why-is-it-58%-of-the-previous-coefficient?"> </a></h4><h4 id="A:-Because-the-standard-deviation-of-$\log-carat$-is-0.58!">A: Because the standard deviation of $\log carat$ is 0.58!<a class="anchor-link" href="#A:-Because-the-standard-deviation-of-$\log-carat$-is-0.58!"> </a></h4><p>This works because, if "$std$" stands for the standard deviation of $\log carat$ each of these steps is valid and doesn't change the estimation:</p>
\begin{align}
\log price &amp; = \hat{8.41} + \hat{1.69} \log carat + \hat{0.10} ideal \\
           &amp; = \hat{8.41} + \hat{1.69} \frac{std}{std} \log carat + \hat{0.10} ideal \\
           &amp; = \hat{8.41} + (\hat{1.69}*std) \frac{\log carat}{std}  + \hat{0.10} ideal 
\end{align}<p>So what that last line shows is that if we divide the variable by its standard deviation, the coefficient will change by an offsetting amount.</p>
<p>If a variable has a small standard deviation (e.g. 0.10), dividing the variable by 0.10 before running the regression will reduce the coefficient by 90%.</p>
<p>Here, $\log carat$ has a deviation of 0.58, so scaling the variable before regressing dropped the</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Goodness-of-fit:-R2-and-Adjusted-R2">Goodness of fit: R2 and Adjusted R2<a class="anchor-link" href="#Goodness-of-fit:-R2-and-Adjusted-R2"> </a></h2><p>The next graph shows two different y variables (shown in blue and orange). Each is "modelled" as a linear function of carats. After we run a regression, we get a prediction for each.</p>
<p>Which line/prediction do you think fits its corresponding data points better? Blue or orange?</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_remove_input">

<div class="cell border-box-sizing code_cell rendered tag_remove_input">

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x29128d2d108&gt;</pre>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../images/04/03_regression_32_1.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Intuitively, the line model "fits" the blue data much better! Your eyes can tell you that the blue data points are much more tightly bunched around the blue line than the orange dots and line are.</p>
<p>One way to summary that idea is via <strong>R2</strong>, aka "R-squared". <strong>R2 measures how much variation in the y variable is explained by the regression line, and ranges from 0 to 1</strong>. Every table above automatically reports R2.</p>
<p>But there are a two massive problems with R2:</p>
<ol>
<li>Including extra X variables can never decrease R2, and in fact, variables that are <em>random noise</em> can increase R2.</li>
<li>Because of (1), optimizing on R2 will quickly lead to including X variables you probably shouldn't. In other words, it will lead to overfitting a model, which means designing a model so specifically to your existing data that it will apply poorly to new data.   </li>
</ol>
<p>This is why you should probably focus on looking at the <strong>Adjusted R2</strong> in regressions, which is simply R2 with a penality for adding additional variables to the model.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Regression-coefficients-have-&quot;feelings&quot;">Regression coefficients have "feelings"<a class="anchor-link" href="#Regression-coefficients-have-&quot;feelings&quot;"> </a></h2><h3 id="OR:-T-stats-and-P-Values">OR: T-stats and P-Values<a class="anchor-link" href="#OR:-T-stats-and-P-Values"> </a></h3><p>Well, not feelings, like bad or good but more like uncertainty.</p>
<p><img src="https://media.giphy.com/media/TIjVQiwWQFDMzjk4gU/giphy.gif" alt=""></p>
<p>The tables above report t-stats and p-values for statistics, which I'm only going to briefly review here. <em>(I'm going to assume you are mostly familiar with these via your pre-reqs. Please let me know if I'm mistaken!)</em></p>
<p>A coefficient's t-stat and p-value can be used to assess the probability that the coefficient is different than zero by random chance.</p>
<ul>
<li>A t-stat of 1.645 corresponds to a p-value of 0.10; meaning only 10% of the time would you get that coefficient randomly</li>
<li>A t-stat of 1.96 corresponds to a p-value of 0.05; this is a common "threshold" to say a "relationship is <em><strong>statistically</strong></em> significant" and that "the relationship between X and y is not zero"</li>
<li>A t-stat of 2.58 corresponds to a p-value of 0.01</li>
</ul>
<p>These thresholds are important and part of a sensible approach to learning from data, but...</p>
<p>When people say a "relationship is statistically significant", what they can mean is 
<br><br></p>
<center>"LOOK YONDER, I HAVE FOUND A NEW, MEANINGFUL  <br>
    CORRELATION THAT IS TRUE <br> (TRULY!) (VERILY!)  <br>
    AND KNOWING IT WILL CHANGE THE WORLD!"</center><p>Which leads me to the next section.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Some-significant-warnings-about-&quot;statistical-significance&quot;">Some <em>significant</em> warnings about "statistical significance"<a class="anchor-link" href="#Some-significant-warnings-about-&quot;statistical-significance&quot;"> </a></h3><p>The focus on p-values can be dangerous. As it turns out, "p-hacking" (finding a significant relationship) is easy for a number of reasons:</p>
<ul>
<li>If you look at enough Xs and enough ys, you, by chance alone, can find "significant" relationships where <a href="https://www.tylervigen.com/spurious-correlations">none exist</a></li>
<li><p>Your data can be biased. Maybe the most famous example:</p>
<p><img src="https://upload.wikimedia.org/wikipedia/en/2/28/Deweytruman12.jpg" alt=""></p>
<p>The newspaper ran a telephone poll in 1948 and found a significant preference for Dewey. The problem? Phones were still expensive in 1948. Their polling method accounted for rural/urban and race, but not income.</p>
<p>Another bias that contributed? The paper had to publish earlier than others due to printing issues, so they used an expert who had been right in 4 of the last 5 elections.</p>
<p><img src="https://media.giphy.com/media/5fBH6zf7l8bxukYh74Q/giphy.gif" alt=""></p>
<p>Turns out a sample size of five wasn't enough to confirm the expert had a crystal ball.</p>
<p><img src="https://media.giphy.com/media/10uct1aSFT7QiY/giphy.gif" alt=""></p>
<p>The notion of turning to people on hot streaks has been pretty roundly debunked in finance (e.g. mutual funds, stock pickers, etc). But my favorite example is <a href="https://www.textbook.ds100.org/ch/18/hyp_phacking.html#Paul-the-Octopus">Paul the Octopus</a>, who picked winners in all seven soccer matches Germany played during the 2010 World Cup.</p>
</li>
<li><p>Your sample restrictions can generate bias: If you evaluate the trading strategy "buy and hold current S&amp;P companies" for the last 50 years, you'll discover that this trading strategy was unbelievable!</p>
</li>
<li><p>Reusing the data: If you torture the data, it will confess</p>
<p>There is a <a href="https://fivethirtyeight.com/features/science-isnt-broken/#part1">fun game in here</a>. The point: The choices you make about what variables to include or focus on can change the <em>statistical</em> results. And if you play with a dataset long enough, you'll find "results"...</p>
</li>
</ul>
<h4 id="The-focus-on-p-values-can-be-dangerous-because-it-distorts-the-incentives-of-researchers:">The focus on p-values can be dangerous because it distorts the incentives of researchers:<a class="anchor-link" href="#The-focus-on-p-values-can-be-dangerous-because-it-distorts-the-incentives-of-researchers:"> </a></h4><ul>
<li>Again: <strong>If you torture the data, it will confess (regardless of whether you or it wanted to)</strong></li>
<li><strong>Motivated reasoning/cognitive dissonance/confirmation bias</strong>: Analysis like <a href="https://fivethirtyeight.com/features/science-isnt-broken/#part1">the game above</a> is fraught with temptations for humans. That 538 article mentions that about 2/3 of retractions are due to misconduct.</li>
<li>Focus on p-value <strong>shifts attention: Statistical signicance does not mean meaningful or economic significance</strong></li>
</ul>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://media.giphy.com/media/nwyqBwP65XCAU/giphy.gif" alt=""></p>
<h3 id="Reading-/-practice-for-this-topic">Reading / practice for this topic<a class="anchor-link" href="#Reading-/-practice-for-this-topic"> </a></h3><ol>
<li>This page!</li>
<li><a href="https://r4ds.had.co.nz/model-intro.html">Chapters 22-24 of R 4 Data Science</a> are an excellent overview of the thought process of modeling</li>
<li>Use <code>statsmodels.api</code> to make nice regression tables by <a href="https://python.quantecon.org/ols.html">following this guide</a> (you can use different data though)</li>
<li>Load the titanic dataset from <code>sns</code> and try some regressions.</li>
</ol>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Acknowledgments">Acknowledgments<a class="anchor-link" href="#Acknowledgments"> </a></h2><ul>
<li>The demo on Diamonds is borrowed from <a href="https://r4ds.had.co.nz">R4DS</a>.</li>
<li><a href="https://www.textbook.ds100.org/intro">DS100</a></li>
<li>Alberto Rossi provided excellent lecture notes  </li>
</ul>

</div>
</div>
</div>
</div>

 


    </main>
    